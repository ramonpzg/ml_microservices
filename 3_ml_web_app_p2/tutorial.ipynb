{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ML Web Apps: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. Tools\n",
    "4. ML Microservices\n",
    "5. Front- and Back-End\n",
    "6. Final Thoughts\n",
    "7. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section focuses on building multiple microservices and combining their usage, in anticipation \n",
    "of a larger application via inference graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demucs\n",
    "- Sentence Transformers\n",
    "- transformers\n",
    "- panns_inference\n",
    "- nicegui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ML Microservices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from pedalboard.io import AudioFile\n",
    "import demucs.api\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with AudioFile(\"05mUf9x3V3RIqafuY4H54E.mp3\", \"r\") as f:\n",
    "    first_song = f.read(f.frames)\n",
    "    first_sample_rate = f.samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_song.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = demucs.api.Separator(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensong = torch.from_numpy(first_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original, results = separator.separate_tensor(tensong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    print(k)\n",
    "    display(Audio(v.numpy(), rate=first_sample_rate))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/splitter/split_model.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "import numpy as np\n",
    "import demucs.api\n",
    "import torch\n",
    "\n",
    "class SongSplitter(MLModel):\n",
    "    async def load(self):\n",
    "        self.separator = demucs.api.Separator(device='cpu')\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, song: np.ndarray) -> np.ndarray:\n",
    "        tensong = torch.from_numpy(song)\n",
    "        original, result = self.separator.separate_tensor(tensong)\n",
    "        return self.post_processor(result)\n",
    "\n",
    "    def post_processor(self, tensor_dict: dict[torch.Tensor]) -> np.ndarray:\n",
    "        tensor_dict[\"instruments\"] = tensor_dict[\"bass\"] + tensor_dict[\"drums\"] + tensor_dict[\"other\"]\n",
    "        del tensor_dict[\"bass\"],  tensor_dict[\"drums\"],  tensor_dict[\"other\"]\n",
    "        return torch.vstack([tensor_dict['vocals'], tensor_dict['instruments']]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/splitter/model-settings.json\n",
    "{\n",
    "    \"name\": \"music_splitter\",\n",
    "    \"implementation\": \"split_model.SongSplitter\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/splitter/settings.json\n",
    "{\n",
    "    \"http_port\": 5010,\n",
    "    \"grpc_port\": 5022,\n",
    "    \"metrics_port\": 5035\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command line, run the following command to start MLServer.\n",
    "\n",
    "```bash\n",
    "mlserver start servers/splitter\n",
    "```\n",
    "\n",
    "Now, let's test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:5010/v2/models/music_splitter/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [\n",
    "        NumpyCodec.encode_input(name=\"song\", payload=first_song).dict()\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(endpoint, json=input_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver.codecs import NumpyCodec\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(song: np.ndarray, sample_rate) -> np.ndarray:\n",
    "    return librosa.resample(\n",
    "        song[0], orig_sr=sample_rate, target_sr=pipe.feature_extractor.sampling_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first = pre_process(first_song, first_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pipe(new_first, max_new_tokens=2000)['text']\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:5060/v2/models/music_transcriber/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [\n",
    "        NumpyCodec.encode_input(name=\"song\", payload=first_song).dict(),\n",
    "        NumpyCodec.encode_input(name=\"sample_rate\", payload=np.array([first_sample_rate])).dict(),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.post(endpoint, json=input_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create a server ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/transcriptor/asr_model.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ASRServer(MLModel):\n",
    "    async def load(self):\n",
    "        self.pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-medium\")\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, song: np.ndarray, sample_rate: np.ndarray) -> List[str]:\n",
    "        resampled_song = self.pre_process(song, sample_rate[0][0])\n",
    "        return [self.pipe(resampled_song, max_new_tokens=2000)['text']]\n",
    "    \n",
    "    def pre_process(self, song: np.ndarray, sample_rate) -> np.ndarray:\n",
    "        return librosa.resample(\n",
    "            song[0], orig_sr=sample_rate, target_sr=self.pipe.feature_extractor.sampling_rate\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before we wrote a `post_processor` but you can also create pre-processing steps in the same \n",
    "fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/transcriptor/model-settings.json\n",
    "{\n",
    "    \"name\": \"music_transcriber\",\n",
    "    \"implementation\": \"asr_model.ASRServer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/transcriptor/settings.json\n",
    "{\n",
    "    \"http_port\": 5060,\n",
    "    \"grpc_port\": 5040,\n",
    "    \"metrics_port\": 5048\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text embeddings are numerical representations of words or phrases used in natural language processing tasks. These representations capture semantic relationships between words, enabling algorithms to understand their meanings. By converting words into dense vectors, text embeddings allow machines to process and analyze textual data more efficiently, making them crucial for tasks like language translation, sentiment analysis, and text similarity comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a Python file called `text_embs.py` with a model definition for the sentence transformers model.\n",
    "2. Create a `settings.json` file with the server configuration.\n",
    "3. Create a `model-settings.json` file with the details of your model.\n",
    "\n",
    "Add all three files to the `./servers/test_embeddings/` directory and start your server once \n",
    "you finish writing the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlserver.codecs import StringCodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:____/v2/models/____/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [\n",
    "        StringCodec.encode_input(name=\"____\", payload=____, use_bytes=False).dict()\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = requests.post(endpoint, json=input_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Inference\n",
    "\n",
    "One nice feature of MLServer that can come in handy at any time is its ability to run a batch inference\n",
    "jobs with a few parameters. Say we have the following group of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sample-text.txt\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"]}]}\n",
    "{\"inputs\": [{\"name\": \"lyrics\", \"shape\": [1, 1], \"datatype\": \"BYTES\", \"parameters\": {\"content_type\": \"str\"}, \"data\": [\"Lorem Ipsum has been the industrys standard dummy text ever since the 1500s when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have the details of our server, we can send requests in parallel using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mlserver infer -u localhost:4080 -m text_embedding -i sample-text.txt -o text-output.txt --workers 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above\n",
    "- sends our file full of requests to the (-u) url `localhost:4080`\n",
    "- to a (-m) model named text_embedding\n",
    "- using the `sample-text.txt` file as (-i) input\n",
    "- and it saves the (-o) result in a file called `text-output.txt`\n",
    "- all of this happens in parallel with 5 (--)workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio embeddings are numerical representations of audio signals, often used in machine learning tasks related to sound and music. These embeddings capture the essential characteristics of audio, such as timbre and rhythm, in a condensed form. By converting audio data into embeddings, it becomes easier for algorithms to process, analyze, and compare different sounds, enabling applications like music recommendation, sound recognition, and audio-based machine learning models.\n",
    "\n",
    "I had fine-tuned a Wave-2-Vec model on the for music genre classification on the [Ludwig Music Dataset (Moods and Subgenres)](https://www.kaggle.com/datasets/jorgeruizdev/ludwig-music-dataset-moods-and-subgenres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(\"audio-classification\", model=\"ramonpzg/wav2musicgenre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('payload.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = df.sample(1)\n",
    "print(random_sample['artist_song'].iloc[0])\n",
    "Audio(url=random_sample['urls'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil_collins = df.loc[df['artist'] == 'Phil Collins', 'artist_song']\n",
    "phil_collins.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_song(metadata, song_to_get):\n",
    "    song_url = metadata.loc[metadata['artist_song'] == song_to_get, 'urls'].iloc[0]\n",
    "    with requests.get(song_url, stream=True) as music:\n",
    "        fil = io.BytesIO(music.content)\n",
    "        with AudioFile(fil, \"r\") as f:\n",
    "            song = f.read(f.frames)\n",
    "            sample_rate = f.samplerate\n",
    "    return song, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil_song, phil_sr = download_song(df, phil_collins.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(phil_song, rate=phil_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(phil_song[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try it our with different songs that you might like best. It is not a perfect model but it \n",
    "works relatively well for this example.\n",
    "\n",
    "Time to build our server! ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/music_cls/music_cls.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MusicClassifier(MLModel):\n",
    "    async def load(self):\n",
    "        self.model = pipeline(\"audio-classification\", model=\"ramonpzg/wav2musicgenre\")\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, song: np.ndarray) -> pd.DataFrame:\n",
    "        result = self.model(song[0])\n",
    "        return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/music_cls/settings.json\n",
    "{\n",
    "    \"http_port\": 5080,\n",
    "    \"grpc_port\": 5060,\n",
    "    \"metrics_port\": 5070\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/music_cls/model-settings.json\n",
    "{\n",
    "    \"name\": \"music_classifier\",\n",
    "    \"implementation\": \"music_cls.MusicClassifier\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:5080/v2/models/music_classifier/infer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to send a single channel to our song so we'll reshape our array into (sample, song_array) \n",
    "in our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phil_song.shape, phil_song[0][None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [\n",
    "        NumpyCodec.encode_input(name=\"song\", payload=phil_song[0][None]).dict(),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embs = requests.post(endpoint, json=input_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embs.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll create a server to classify the emotions of the transcribed songs.\n",
    "\n",
    "We will be use the [Roberta Base Go Emotions](https://huggingface.co/SamLowe/roberta-base-go_emotions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentiment_pipeline(trans)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/sentiment/emotions.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "class EmotionClassifier(MLModel):\n",
    "    async def load(self):\n",
    "        self.model = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(self, lyrics: List[str]) -> pd.DataFrame:\n",
    "        result = self.model(lyrics)\n",
    "        return pd.DataFrame(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/sentiment/model-settings.json\n",
    "{\n",
    "    \"name\": \"sentiformer\",\n",
    "    \"implementation\": \"emotions.EmotionClassifier\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile servers/sentiment/settings.json\n",
    "{\n",
    "    \"http_port\": 5010,\n",
    "    \"grpc_port\": 5020,\n",
    "    \"metrics_port\": 5018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://localhost:5010/v2/models/sentiformer/infer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_request = {\n",
    "    \"inputs\": [\n",
    "        StringCodec.encode_input(name=\"lyrics\", payload=[trans], use_bytes=False).dict()\n",
    "    ]\n",
    "}\n",
    "input_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = requests.post(endpoint, json=input_request)\n",
    "sentiment.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Front-End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will walk through the file called `main.py` in the current directory. It contains a `nicegui` \n",
    "application with a few moving parts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to showcase any of the audio transcription of a song.\n",
    "You will need, a widget and a callback to send requests to the server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_svcs_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
